{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand28/qiuandr1/envs/csc413/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "from model import TwoResAutoEncoder\n",
    "from data import FaceDataset, JpgBeforeAfterDataset\n",
    "from train import create_fg_masks\n",
    "import matplotlib.pyplot as plt\n",
    "from feather import stitch\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title LPIPS\n",
    "# !git clone https://github.com/richzhang/PerceptualSimilarity\n",
    "# !mv PerceptualSimilarity/lpips/weights weights\n",
    "# !rm -rf PerceptualSimilarity\n",
    "\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from torchvision import models as tv\n",
    "\n",
    "# def psnr(p0, p1, peak=255.):\n",
    "#     return 10*np.log10(peak**2/np.mean((1.*p0-1.*p1)**2))\n",
    "\n",
    "def psnr(p0, p1, peak=255.):\n",
    "    return 10 * torch.log10(peak ** 2 / torch.mean((p0 - p1) ** 2))\n",
    "\n",
    "\n",
    "def normalize_tensor(in_feat,eps=1e-10):\n",
    "    norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1,keepdim=True))\n",
    "    return in_feat/(norm_factor+eps)\n",
    "\n",
    "def l2(p0, p1, range=255.):\n",
    "    return .5*np.mean((p0 / range - p1 / range)**2)\n",
    "\n",
    "\n",
    "def tensor2np(tensor_obj):\n",
    "    # change dimension of a tensor object into a numpy array\n",
    "    return tensor_obj[0].cpu().float().numpy().transpose((1,2,0))\n",
    "\n",
    "def np2tensor(np_obj):\n",
    "     # change dimenion of np array into tensor array\n",
    "    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n",
    "\n",
    "def tensor2tensorlab(image_tensor,to_norm=True,mc_only=False):\n",
    "    # image tensor to lab tensor\n",
    "    from skimage import color\n",
    "\n",
    "    img = tensor2im(image_tensor)\n",
    "    img_lab = color.rgb2lab(img)\n",
    "    if(mc_only):\n",
    "        img_lab[:,:,0] = img_lab[:,:,0]-50\n",
    "    if(to_norm and not mc_only):\n",
    "        img_lab[:,:,0] = img_lab[:,:,0]-50\n",
    "        img_lab = img_lab/100.\n",
    "\n",
    "    return np2tensor(img_lab)\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n",
    "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "class squeezenet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(squeezenet, self).__init__()\n",
    "        pretrained_features = tv.squeezenet1_1(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.slice6 = torch.nn.Sequential()\n",
    "        self.slice7 = torch.nn.Sequential()\n",
    "        self.N_slices = 7\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(2,5):\n",
    "            self.slice2.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(5, 8):\n",
    "            self.slice3.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(8, 10):\n",
    "            self.slice4.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(10, 11):\n",
    "            self.slice5.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(11, 12):\n",
    "            self.slice6.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(12, 13):\n",
    "            self.slice7.add_module(str(x), pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        h = self.slice6(h)\n",
    "        h_relu6 = h\n",
    "        h = self.slice7(h)\n",
    "        h_relu7 = h\n",
    "        vgg_outputs = namedtuple(\"SqueezeOutputs\", ['relu1','relu2','relu3','relu4','relu5','relu6','relu7'])\n",
    "        out = vgg_outputs(h_relu1,h_relu2,h_relu3,h_relu4,h_relu5,h_relu6,h_relu7)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class alexnet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(alexnet, self).__init__()\n",
    "        alexnet_pretrained_features = tv.alexnet(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(2, 5):\n",
    "            self.slice2.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(5, 8):\n",
    "            self.slice3.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(8, 10):\n",
    "            self.slice4.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(10, 12):\n",
    "            self.slice5.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        alexnet_outputs = namedtuple(\"AlexnetOutputs\", ['relu1', 'relu2', 'relu3', 'relu4', 'relu5'])\n",
    "        out = alexnet_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
    "\n",
    "        return out\n",
    "\n",
    "class vgg16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(vgg16, self).__init__()\n",
    "        vgg_pretrained_features = tv.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class resnet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True, num=18):\n",
    "        super(resnet, self).__init__()\n",
    "        if(num==18):\n",
    "            self.net = tv.resnet18(pretrained=pretrained)\n",
    "        elif(num==34):\n",
    "            self.net = tv.resnet34(pretrained=pretrained)\n",
    "        elif(num==50):\n",
    "            self.net = tv.resnet50(pretrained=pretrained)\n",
    "        elif(num==101):\n",
    "            self.net = tv.resnet101(pretrained=pretrained)\n",
    "        elif(num==152):\n",
    "            self.net = tv.resnet152(pretrained=pretrained)\n",
    "        self.N_slices = 5\n",
    "\n",
    "        self.conv1 = self.net.conv1\n",
    "        self.bn1 = self.net.bn1\n",
    "        self.relu = self.net.relu\n",
    "        self.maxpool = self.net.maxpool\n",
    "        self.layer1 = self.net.layer1\n",
    "        self.layer2 = self.net.layer2\n",
    "        self.layer3 = self.net.layer3\n",
    "        self.layer4 = self.net.layer4\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.conv1(X)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        h_relu1 = h\n",
    "        h = self.maxpool(h)\n",
    "        h = self.layer1(h)\n",
    "        h_conv2 = h\n",
    "        h = self.layer2(h)\n",
    "        h_conv3 = h\n",
    "        h = self.layer3(h)\n",
    "        h_conv4 = h\n",
    "        h = self.layer4(h)\n",
    "        h_conv5 = h\n",
    "\n",
    "        outputs = namedtuple(\"Outputs\", ['relu1','conv2','conv3','conv4','conv5'])\n",
    "        out = outputs(h_relu1, h_conv2, h_conv3, h_conv4, h_conv5)\n",
    "\n",
    "        return out\n",
    "\n",
    "def upsample(in_tens, out_HW=(64,64)): # assumes scale factor is same for H and W\n",
    "    in_H, in_W = in_tens.shape[2], in_tens.shape[3]\n",
    "    return nn.Upsample(size=out_HW, mode='bilinear', align_corners=False)(in_tens)\n",
    "\n",
    "# Learned perceptual metric\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, pretrained=True, net='squeeze', version='0.1', lpips=True, spatial=False,\n",
    "        pnet_rand=False, pnet_tune=False, use_dropout=True, model_path=None, eval_mode=True, verbose=True):\n",
    "        \"\"\" Initializes a perceptual loss torch.nn.Module\n",
    "\n",
    "        Parameters (default listed first)\n",
    "        ---------------------------------\n",
    "        lpips : bool\n",
    "            [True] use linear layers on top of base/trunk network\n",
    "            [False] means no linear layers; each layer is averaged together\n",
    "        pretrained : bool\n",
    "            This flag controls the linear layers, which are only in effect when lpips=True above\n",
    "            [True] means linear layers are calibrated with human perceptual judgments\n",
    "            [False] means linear layers are randomly initialized\n",
    "        pnet_rand : bool\n",
    "            [False] means trunk loaded with ImageNet classification weights\n",
    "            [True] means randomly initialized trunk\n",
    "        net : str\n",
    "            ['alex','vgg','squeeze'] are the base/trunk networks available\n",
    "        version : str\n",
    "            ['v0.1'] is the default and latest\n",
    "            ['v0.0'] contained a normalization bug; corresponds to old arxiv v1 (https://arxiv.org/abs/1801.03924v1)\n",
    "        model_path : 'str'\n",
    "            [None] is default and loads the pretrained weights from paper https://arxiv.org/abs/1801.03924v1\n",
    "\n",
    "        The following parameters should only be changed if training the network\n",
    "\n",
    "        eval_mode : bool\n",
    "            [True] is for test mode (default)\n",
    "            [False] is for training mode\n",
    "        pnet_tune\n",
    "            [False] keep base/trunk frozen\n",
    "            [True] tune the base/trunk network\n",
    "        use_dropout : bool\n",
    "            [True] to use dropout when training linear layers\n",
    "            [False] for no dropout when training linear layers\n",
    "        \"\"\"\n",
    "\n",
    "        super(LPIPS, self).__init__()\n",
    "        if(verbose):\n",
    "            print('Setting up [%s] perceptual loss: trunk [%s], v[%s], spatial [%s]'%\n",
    "                ('LPIPS' if lpips else 'baseline', net, version, 'on' if spatial else 'off'))\n",
    "\n",
    "        self.pnet_type = net\n",
    "        self.pnet_tune = pnet_tune\n",
    "        self.pnet_rand = pnet_rand\n",
    "        self.spatial = spatial\n",
    "        self.lpips = lpips # false means baseline of just averaging all layers\n",
    "        self.version = version\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "\n",
    "        if(self.pnet_type in ['vgg','vgg16']):\n",
    "            net_type = vgg16\n",
    "            self.chns = [64,128,256,512,512]\n",
    "        elif(self.pnet_type=='alex'):\n",
    "            net_type = alexnet\n",
    "            self.chns = [64,192,384,256,256]\n",
    "        elif(self.pnet_type=='squeeze'):\n",
    "            net_type = squeezenet\n",
    "            self.chns = [64,128,256,384,384,512,512]\n",
    "        self.L = len(self.chns)\n",
    "\n",
    "        self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)\n",
    "\n",
    "        if(lpips):\n",
    "            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "            self.lins = [self.lin0,self.lin1,self.lin2,self.lin3,self.lin4]\n",
    "            if(self.pnet_type=='squeeze'): # 7 layers for squeezenet\n",
    "                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)\n",
    "                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)\n",
    "                self.lins+=[self.lin5,self.lin6]\n",
    "            self.lins = nn.ModuleList(self.lins)\n",
    "\n",
    "            if(pretrained):\n",
    "                if(model_path is None):\n",
    "                    # import inspect\n",
    "                    # import os\n",
    "                    # model_path = os.path.abspath(os.path.join(inspect.getfile(self.__init__), '..', 'weights/v%s/%s.pth'%(version,net)))\n",
    "                    model_path = os.path.abspath(os.path.join('./', 'weights/v%s/%s.pth'%(version,net)))\n",
    "\n",
    "                if(verbose):\n",
    "                    print('Loading model from: %s'%model_path)\n",
    "                self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "\n",
    "        if(eval_mode):\n",
    "            self.eval()\n",
    "\n",
    "    def forward(self, in0, in1, retPerLayer=False, normalize=False, mask=None):\n",
    "        return lpips_forward(self, in0, in1, retPerLayer, normalize, mask)\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer('shift', torch.Tensor([-.030,-.088,-.188])[None,:,None,None])\n",
    "        self.register_buffer('scale', torch.Tensor([.458,.448,.450])[None,:,None,None])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    ''' A single linear layer which does a 1x1 conv '''\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "\n",
    "        layers = [nn.Dropout(),] if(use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Dist2LogitLayer(nn.Module):\n",
    "    ''' takes 2 distances, puts through fc layers, spits out value between [0,1] (if use_sigmoid is True) '''\n",
    "    def __init__(self, chn_mid=32, use_sigmoid=True):\n",
    "        super(Dist2LogitLayer, self).__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True),]\n",
    "        layers += [nn.LeakyReLU(0.2,True),]\n",
    "        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1, padding=0, bias=True),]\n",
    "        layers += [nn.LeakyReLU(0.2,True),]\n",
    "        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True),]\n",
    "        if(use_sigmoid):\n",
    "            layers += [nn.Sigmoid(),]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,d0,d1,eps=0.1):\n",
    "        return self.model.forward(torch.cat((d0,d1,d0-d1,d0/(d1+eps),d1/(d0+eps)),dim=1))\n",
    "\n",
    "class BCERankingLoss(nn.Module):\n",
    "    def __init__(self, chn_mid=32):\n",
    "        super(BCERankingLoss, self).__init__()\n",
    "        self.net = Dist2LogitLayer(chn_mid=chn_mid)\n",
    "        # self.parameters = list(self.net.parameters())\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, d0, d1, judge):\n",
    "        per = (judge+1.)/2.\n",
    "        self.logit = self.net.forward(d0,d1)\n",
    "        return self.loss(self.logit, per)\n",
    "\n",
    "\n",
    "def spatial_average(in_tens, keepdim=True, mask=None):\n",
    "    if mask is not None:\n",
    "        resizing = Resize((in_tens.shape[2], in_tens.shape[3]))\n",
    "        resized_mask = resizing(mask)\n",
    "\n",
    "        # fig, ax = plt.subplots(1, 3)\n",
    "        # fig.set_figwidth(15)\n",
    "        # fig.set_figheight(7)\n",
    "\n",
    "        # img = ax[0].imshow(in_tens[0, 0].cpu().detach())\n",
    "        # fig.colorbar(img)\n",
    "        # img = ax[1].imshow(resized_mask[0, 0].cpu().detach())\n",
    "        # fig.colorbar(img)\n",
    "\n",
    "        in_tens = resized_mask * in_tens\n",
    "        # img = ax[2].imshow(in_tens[0, 0].cpu().detach())\n",
    "        # fig.colorbar(img)\n",
    "        # fig.show()\n",
    "        return torch.sum(in_tens, dim=(2, 3), keepdim=keepdim) / torch.sum(resized_mask, dim=(2, 3))\n",
    "\n",
    "    return in_tens.mean([2,3],keepdim=keepdim)\n",
    "\n",
    "def lpips_forward(self, in0, in1, retPerLayer=False, normalize=False, mask=None):\n",
    "    if normalize: # turn on this flag if input is [0,1] so it can be adjusted to [-1, +1]\n",
    "        in0 = 2 * in0  - 1\n",
    "        in1 = 2 * in1  - 1\n",
    "\n",
    "    # v0.0 - original release had a bug, where input was not scaled\n",
    "    in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version=='0.1' else (in0, in1)\n",
    "    outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "    feats0, feats1, diffs = {}, {}, {}\n",
    "\n",
    "    for kk in range(self.L):\n",
    "        feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n",
    "        diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "    if(self.lpips):\n",
    "        if(self.spatial):\n",
    "            res = [upsample(self.lins[kk](diffs[kk]), out_HW=in0.shape[2:]) for kk in range(self.L)]\n",
    "        else:\n",
    "            res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True, mask=mask) for kk in range(self.L)]\n",
    "    else:\n",
    "        if(self.spatial):\n",
    "            res = [upsample(diffs[kk].sum(dim=1,keepdim=True), out_HW=in0.shape[2:]) for kk in range(self.L)]\n",
    "        else:\n",
    "            res = [spatial_average(diffs[kk].sum(dim=1,keepdim=True), keepdim=True, mask=mask) for kk in range(self.L)]\n",
    "\n",
    "    val = 0\n",
    "    for l in range(self.L):\n",
    "        val += res[l]\n",
    "\n",
    "    if(retPerLayer):\n",
    "        return (val, res)\n",
    "    else:\n",
    "        return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MS-SSIM\n",
    "\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def _fspecial_gauss_1d(size: int, sigma: float) -> Tensor:\n",
    "    r\"\"\"Create 1-D gauss kernel\n",
    "    Args:\n",
    "        size (int): the size of gauss kernel\n",
    "        sigma (float): sigma of normal distribution\n",
    "    Returns:\n",
    "        torch.Tensor: 1D kernel (1 x 1 x size)\n",
    "    \"\"\"\n",
    "    coords = torch.arange(size, dtype=torch.float)\n",
    "    coords -= size // 2\n",
    "\n",
    "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "    g /= g.sum()\n",
    "\n",
    "    return g.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def gaussian_filter(input: Tensor, win: Tensor) -> Tensor:\n",
    "    r\"\"\" Blur input with 1-D kernel\n",
    "    Args:\n",
    "        input (torch.Tensor): a batch of tensors to be blurred\n",
    "        window (torch.Tensor): 1-D gauss kernel\n",
    "    Returns:\n",
    "        torch.Tensor: blurred tensors\n",
    "    \"\"\"\n",
    "    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n",
    "    if len(input.shape) == 4:\n",
    "        conv = F.conv2d\n",
    "    elif len(input.shape) == 5:\n",
    "        conv = F.conv3d\n",
    "    else:\n",
    "        raise NotImplementedError(input.shape)\n",
    "\n",
    "    C = input.shape[1]\n",
    "    out = input\n",
    "    for i, s in enumerate(input.shape[2:]):\n",
    "        if s >= win.shape[-1]:\n",
    "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float,\n",
    "    win: Tensor,\n",
    "    size_average: bool = True,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    mask: Tensor = None\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\" Calculate ssim index for X and Y\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): images\n",
    "        Y (torch.Tensor): images\n",
    "        data_range (float or int): value range of input images. (usually 1.0 or 255)\n",
    "        win (torch.Tensor): 1-D gauss kernel\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: ssim results.\n",
    "    \"\"\"\n",
    "    K1, K2 = K\n",
    "    # batch, channel, [depth,] height, width = X.shape\n",
    "    compensation = 1.0\n",
    "\n",
    "    C1 = (K1 * data_range) ** 2\n",
    "    C2 = (K2 * data_range) ** 2\n",
    "\n",
    "    win = win.to(X.device, dtype=X.dtype)\n",
    "\n",
    "    mu1 = gaussian_filter(X, win)\n",
    "    mu2 = gaussian_filter(Y, win)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n",
    "    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n",
    "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n",
    "\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n",
    "\n",
    "    if mask is not None:\n",
    "        resizing = Resize(cs_map.shape[-2:])\n",
    "        weights = resizing(mask).to(device).float()\n",
    "        cs_map = 2 * (((cs_map + 1) / 2) ** weights) - 1\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "\n",
    "    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n",
    "    cs = torch.flatten(cs_map, 2).mean(-1)\n",
    "    return ssim_per_channel, cs\n",
    "\n",
    "\n",
    "def ms_ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float = 255,\n",
    "    size_average: bool = True,\n",
    "    win_size: int = 11,\n",
    "    win_sigma: float = 1.5,\n",
    "    win: Optional[Tensor] = None,\n",
    "    weights: Optional[List[float]] = None,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    mask: Tensor = None\n",
    ") -> Tensor:\n",
    "    r\"\"\" interface of ms-ssim\n",
    "    Args:\n",
    "        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        weights (list, optional): weights for different levels\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "    Returns:\n",
    "        torch.Tensor: ms-ssim results\n",
    "    \"\"\"\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
    "\n",
    "    for d in range(len(X.shape) - 1, 1, -1):\n",
    "        X = X.squeeze(dim=d)\n",
    "        Y = Y.squeeze(dim=d)\n",
    "\n",
    "    #if not X.type() == Y.type():\n",
    "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
    "\n",
    "    if len(X.shape) == 4:\n",
    "        avg_pool = F.avg_pool2d\n",
    "    elif len(X.shape) == 5:\n",
    "        avg_pool = F.avg_pool3d\n",
    "    else:\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
    "\n",
    "    if win is not None:  # set win_size\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError(\"Window size should be odd.\")\n",
    "\n",
    "    smaller_side = min(X.shape[-2:])\n",
    "    assert smaller_side > (win_size - 1) * (\n",
    "        2 ** 4\n",
    "    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "    weights_tensor = X.new_tensor(weights)\n",
    "\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
    "\n",
    "    levels = weights_tensor.shape[0]\n",
    "    mcs = []\n",
    "    for i in range(levels):\n",
    "        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K, mask=mask)\n",
    "        mask = None\n",
    "\n",
    "        if i < levels - 1:\n",
    "            mcs.append(torch.relu(cs))\n",
    "            padding = [s % 2 for s in X.shape[2:]]\n",
    "            X = avg_pool(X, kernel_size=2, padding=padding)\n",
    "            Y = avg_pool(Y, kernel_size=2, padding=padding)\n",
    "\n",
    "    ssim_per_channel = torch.relu(ssim_per_channel)  # type: ignore  # (batch, channel)\n",
    "    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n",
    "    ms_ssim_val = torch.prod(mcs_and_ssim ** weights_tensor.view(-1, 1, 1), dim=0)\n",
    "\n",
    "    if size_average:\n",
    "        return ms_ssim_val.mean()\n",
    "    else:\n",
    "        return ms_ssim_val.mean(1)\n",
    "\n",
    "class MS_SSIM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range: float = 255,\n",
    "        size_average: bool = True,\n",
    "        win_size: int = 11,\n",
    "        win_sigma: float = 1.5,\n",
    "        channel: int = 3,\n",
    "        spatial_dims: int = 2,\n",
    "        weights: Optional[List[float]] = None,\n",
    "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    ) -> None:\n",
    "        r\"\"\" class for ms-ssim\n",
    "        Args:\n",
    "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "            win_size: (int, optional): the size of gauss kernel\n",
    "            win_sigma: (float, optional): sigma of normal distribution\n",
    "            channel (int, optional): input channels (default: 3)\n",
    "            weights (list, optional): weights for different levels\n",
    "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MS_SSIM, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.weights = weights\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, X: Tensor, Y: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        return ms_ssim(\n",
    "            X,\n",
    "            Y,\n",
    "            data_range=self.data_range,\n",
    "            size_average=self.size_average,\n",
    "            win=self.win,\n",
    "            weights=self.weights,\n",
    "            K=self.K,\n",
    "            mask=mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand28/qiuandr1/envs/csc413/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/ondemand28/qiuandr1/envs/csc413/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /nfs/horai.dgpsrv/ondemand28/qiuandr1/CSC413/compression/boiNet/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "lpips_alex = LPIPS(net='alex').to(device)\n",
    "# lpips_squeeze = LPIPS(net='squeeze').to(device)\n",
    "# lpips_vgg = LPIPS(net='vgg').to(device)\n",
    "\n",
    "lpips_alex_loss = lambda x, y: lpips_alex(2 * x - 1, 2 * y - 1)\n",
    "# lpips_squeeze_loss = lambda x, y: lpips_squeeze(2 * x - 1, 2 * y - 1)\n",
    "# lpips_vgg_loss = lambda x, y: lpips_vgg(2 * x - 1, 2 * y - 1)\n",
    "\n",
    "msssim = MS_SSIM(data_range=1, size_average=True, channel=3)\n",
    "l2 = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "    \n",
    "def evaluate_loss(model: nn.Module, dataloader, amount: int=-1, is_two_autoencoder: bool = True) -> None:\n",
    "    loss_l2 = 0\n",
    "    loss_l1 = 0\n",
    "    loss_alex = 0\n",
    "    loss_squeeze = 0\n",
    "    loss_vgg = 0\n",
    "    loss_msssim = 0\n",
    "    loss_psnr = 0\n",
    "\n",
    "    for i, (imgs, faces, bboxes) in enumerate(tqdm(dataloader)):\n",
    "        if i == amount:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if is_two_autoencoder:\n",
    "                imgs, faces = imgs.to(device), faces.to(device)\n",
    "                \n",
    "                fg_masks = create_fg_masks(bboxes).to(device)\n",
    "                fg_output, bg_output = model(imgs * (~fg_masks), faces)\n",
    "                \n",
    "                imgs_reconstruction = stitch(fg_output, bg_output,\n",
    "                                             bboxes.to(device), feather_size=20, device=device)\n",
    "            else:\n",
    "                imgs = imgs.to(device)\n",
    "                \n",
    "                imgs_reconstruction = model(imgs)\n",
    "            \n",
    "            loss_l2 += l2(imgs, imgs_reconstruction)\n",
    "            loss_l1 += l1(imgs, imgs_reconstruction)\n",
    "            loss_alex += lpips_alex_loss(imgs, imgs_reconstruction).sum()\n",
    "            # loss_squeeze += lpips_squeeze_loss(imgs, imgs_reconstruction)\n",
    "            # loss_vgg += lpips_vgg_loss(imgs, imgs_reconstruction)\n",
    "            loss_msssim += msssim(imgs, imgs_reconstruction)\n",
    "            loss_psnr += psnr(imgs, imgs_reconstruction, peak=1)\n",
    "\n",
    "        del imgs, imgs_reconstruction\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    n = max(len(dataloader), amount)\n",
    "    process = lambda x: round(x.item() / n, 5)\n",
    "\n",
    "    loss_l2 = process(loss_l2)\n",
    "    loss_l1 = process(loss_l1)\n",
    "    loss_alex = process(loss_alex)\n",
    "    # loss_squeeze = process(torch.mean(loss_squeeze))\n",
    "    # loss_vgg = process(torch.mean(loss_vgg))\n",
    "    loss_msssim = process(loss_msssim)\n",
    "    loss_psnr = process(loss_psnr)\n",
    "\n",
    "    print(f\"MODEL EVALUATION\")\n",
    "    print(f\"----------------\")\n",
    "    print(f\"{loss_l2}\\tL2\")\n",
    "    print(f\"{loss_l1}\\tL1\")\n",
    "    print(f\"{loss_alex}\\tLPIPS-Alex\")\n",
    "    # print(f\"{loss_squeeze}\\tLPIPS-Squeeze\")\n",
    "    # print(f\"{loss_vgg}\\tLPIPS-VGG\")\n",
    "    print(f\"{loss_msssim}\\tMS-SSIM\")\n",
    "    print(f\"{loss_psnr}\\tPSNR\")\n",
    "    \n",
    "    \n",
    "def evaluate_loss_no_model(dataloader, amount: int=-1) -> None:\n",
    "    loss_l2 = 0\n",
    "    loss_l1 = 0\n",
    "    loss_alex = 0\n",
    "    # loss_squeeze = 0\n",
    "    # loss_vgg = 0\n",
    "    loss_msssim = 0\n",
    "    loss_psnr = 0\n",
    "\n",
    "    for i, (imgs, imgs_reconstruction) in enumerate(tqdm(dataloader)):        \n",
    "        if i == amount:\n",
    "            break\n",
    "\n",
    "        imgs, imgs_reconstruction = imgs.to(device), imgs_reconstruction.to(device)\n",
    "        \n",
    "        loss_l2 += l2(imgs, imgs_reconstruction)\n",
    "        loss_l1 += l1(imgs, imgs_reconstruction)\n",
    "        loss_alex += lpips_alex_loss(imgs, imgs_reconstruction).sum()\n",
    "        # loss_squeeze += lpips_squeeze_loss(imgs, imgs_reconstruction)\n",
    "        # loss_vgg += lpips_vgg_loss(imgs, imgs_reconstruction)\n",
    "        loss_msssim += msssim(imgs, imgs_reconstruction)\n",
    "        loss_psnr += psnr(imgs, imgs_reconstruction, peak=1)\n",
    "\n",
    "        del imgs, imgs_reconstruction\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    n = max(len(dataloader), amount)\n",
    "    process = lambda x: round(x.item() / n, 5)\n",
    "\n",
    "    loss_l2 = process(loss_l2)\n",
    "    loss_l1 = process(loss_l1)\n",
    "    loss_alex = process(loss_alex)\n",
    "    # loss_squeeze = process(torch.mean(loss_squeeze))\n",
    "    # loss_vgg = process(torch.mean(loss_vgg))\n",
    "    loss_msssim = process(loss_msssim)\n",
    "    loss_psnr = process(loss_psnr)\n",
    "\n",
    "    print(f\"MODEL EVALUATION\")\n",
    "    print(f\"----------------\")\n",
    "    print(f\"{loss_l2}\\tL2\")\n",
    "    print(f\"{loss_l1}\\tL1\")\n",
    "    print(f\"{loss_alex}\\tLPIPS-Alex\")\n",
    "    # print(f\"{loss_squeeze}\\tLPIPS-Squeeze\")\n",
    "    # print(f\"{loss_vgg}\\tLPIPS-VGG\")\n",
    "    print(f\"{loss_msssim}\\tMS-SSIM\")\n",
    "    print(f\"{loss_psnr}\\tPSNR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filenames = [filename for filename in os.listdir('.') if filename.endswith('.pt')]\n",
    "\n",
    "# dataset = FaceDataset('test_set_bb.csv', '../celeba/img_align_celeba', return_im_num=False)\n",
    "# dataloader = DataLoader(dataset, batch_size=1024)\n",
    "\n",
    "# for filename in model_filenames:\n",
    "#     print(filename)\n",
    "#     model = torch.load(filename)\n",
    "#     model.to(device)\n",
    "\n",
    "#     evaluate_loss(model, dataloader, is_two_autoencoder=not 'Base' in filename)\n",
    "    \n",
    "#     del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 294/313 [37:13<02:24,  7.60s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 13.06 MiB is free. Process 105123 has 9.90 GiB memory in use. Including non-PyTorch memory, this process has 13.71 GiB memory in use. Of the allocated memory 12.85 GiB is allocated by PyTorch, and 674.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m JpgBeforeAfterDataset(before_dir, after_dir)\n\u001b[1;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluate_loss_no_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mevaluate_loss_no_model\u001b[0;34m(dataloader, amount)\u001b[0m\n\u001b[1;32m     88\u001b[0m loss_l2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l2(imgs, imgs_reconstruction)\n\u001b[1;32m     89\u001b[0m loss_l1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l1(imgs, imgs_reconstruction)\n\u001b[0;32m---> 90\u001b[0m loss_alex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlpips_alex_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs_reconstruction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# loss_squeeze += lpips_squeeze_loss(imgs, imgs_reconstruction)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# loss_vgg += lpips_vgg_loss(imgs, imgs_reconstruction)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m loss_msssim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m msssim(imgs, imgs_reconstruction)\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m lpips_alex \u001b[38;5;241m=\u001b[39m LPIPS(net\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malex\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# lpips_squeeze = LPIPS(net='squeeze').to(device)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# lpips_vgg = LPIPS(net='vgg').to(device)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m lpips_alex_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[43mlpips_alex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# lpips_squeeze_loss = lambda x, y: lpips_squeeze(2 * x - 1, 2 * y - 1)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# lpips_vgg_loss = lambda x, y: lpips_vgg(2 * x - 1, 2 * y - 1)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m msssim \u001b[38;5;241m=\u001b[39m MS_SSIM(data_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/scratch/ondemand28/qiuandr1/envs/csc413/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ondemand28/qiuandr1/envs/csc413/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 330\u001b[0m, in \u001b[0;36mLPIPS.forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize, mask)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in0, in1, retPerLayer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlpips_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretPerLayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 418\u001b[0m, in \u001b[0;36mlpips_forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize, mask)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL):\n\u001b[1;32m    417\u001b[0m     feats0[kk], feats1[kk] \u001b[38;5;241m=\u001b[39m normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n\u001b[0;32m--> 418\u001b[0m     diffs[kk] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mfeats0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeats1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlpips):\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial):\n",
      "File \u001b[0;32m/scratch/ondemand28/qiuandr1/envs/csc413/lib/python3.10/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 13.06 MiB is free. Process 105123 has 9.90 GiB memory in use. Including non-PyTorch memory, this process has 13.71 GiB memory in use. Of the allocated memory 12.85 GiB is allocated by PyTorch, and 674.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "before_dir = '../celeba/img_align_celeba' # \n",
    "after_dir = '../compressed_jpg'\n",
    "\n",
    "dataset = JpgBeforeAfterDataset(before_dir, after_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "evaluate_loss_no_model(dataloader, amount=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
