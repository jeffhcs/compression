{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "from model import TwoResAutoEncoder, QuantizedTwoResAutoEncoder\n",
    "from data import FaceDataset, JpgBeforeAfterDataset\n",
    "from train import create_fg_masks\n",
    "import matplotlib.pyplot as plt\n",
    "from feather import stitch\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title LPIPS\n",
    "!git clone https://github.com/richzhang/PerceptualSimilarity\n",
    "!mv PerceptualSimilarity/lpips/weights weights\n",
    "!rm -rf PerceptualSimilarity\n",
    "\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from torchvision import models as tv\n",
    "\n",
    "# def psnr(p0, p1, peak=255.):\n",
    "#     return 10*np.log10(peak**2/np.mean((1.*p0-1.*p1)**2))\n",
    "\n",
    "def psnr(p0, p1, peak=255.):\n",
    "    return 10 * torch.log10(peak ** 2 / torch.mean((p0 - p1) ** 2))\n",
    "\n",
    "\n",
    "def normalize_tensor(in_feat,eps=1e-10):\n",
    "    norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1,keepdim=True))\n",
    "    return in_feat/(norm_factor+eps)\n",
    "\n",
    "def l2(p0, p1, range=255.):\n",
    "    return .5*np.mean((p0 / range - p1 / range)**2)\n",
    "\n",
    "\n",
    "def tensor2np(tensor_obj):\n",
    "    # change dimension of a tensor object into a numpy array\n",
    "    return tensor_obj[0].cpu().float().numpy().transpose((1,2,0))\n",
    "\n",
    "def np2tensor(np_obj):\n",
    "     # change dimenion of np array into tensor array\n",
    "    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n",
    "\n",
    "def tensor2tensorlab(image_tensor,to_norm=True,mc_only=False):\n",
    "    # image tensor to lab tensor\n",
    "    from skimage import color\n",
    "\n",
    "    img = tensor2im(image_tensor)\n",
    "    img_lab = color.rgb2lab(img)\n",
    "    if(mc_only):\n",
    "        img_lab[:,:,0] = img_lab[:,:,0]-50\n",
    "    if(to_norm and not mc_only):\n",
    "        img_lab[:,:,0] = img_lab[:,:,0]-50\n",
    "        img_lab = img_lab/100.\n",
    "\n",
    "    return np2tensor(img_lab)\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n",
    "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "class squeezenet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(squeezenet, self).__init__()\n",
    "        pretrained_features = tv.squeezenet1_1(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.slice6 = torch.nn.Sequential()\n",
    "        self.slice7 = torch.nn.Sequential()\n",
    "        self.N_slices = 7\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(2,5):\n",
    "            self.slice2.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(5, 8):\n",
    "            self.slice3.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(8, 10):\n",
    "            self.slice4.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(10, 11):\n",
    "            self.slice5.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(11, 12):\n",
    "            self.slice6.add_module(str(x), pretrained_features[x])\n",
    "        for x in range(12, 13):\n",
    "            self.slice7.add_module(str(x), pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        h = self.slice6(h)\n",
    "        h_relu6 = h\n",
    "        h = self.slice7(h)\n",
    "        h_relu7 = h\n",
    "        vgg_outputs = namedtuple(\"SqueezeOutputs\", ['relu1','relu2','relu3','relu4','relu5','relu6','relu7'])\n",
    "        out = vgg_outputs(h_relu1,h_relu2,h_relu3,h_relu4,h_relu5,h_relu6,h_relu7)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class alexnet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(alexnet, self).__init__()\n",
    "        alexnet_pretrained_features = tv.alexnet(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(2, 5):\n",
    "            self.slice2.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(5, 8):\n",
    "            self.slice3.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(8, 10):\n",
    "            self.slice4.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        for x in range(10, 12):\n",
    "            self.slice5.add_module(str(x), alexnet_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5 = h\n",
    "        alexnet_outputs = namedtuple(\"AlexnetOutputs\", ['relu1', 'relu2', 'relu3', 'relu4', 'relu5'])\n",
    "        out = alexnet_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
    "\n",
    "        return out\n",
    "\n",
    "class vgg16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(vgg16, self).__init__()\n",
    "        vgg_pretrained_features = tv.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class resnet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True, num=18):\n",
    "        super(resnet, self).__init__()\n",
    "        if(num==18):\n",
    "            self.net = tv.resnet18(pretrained=pretrained)\n",
    "        elif(num==34):\n",
    "            self.net = tv.resnet34(pretrained=pretrained)\n",
    "        elif(num==50):\n",
    "            self.net = tv.resnet50(pretrained=pretrained)\n",
    "        elif(num==101):\n",
    "            self.net = tv.resnet101(pretrained=pretrained)\n",
    "        elif(num==152):\n",
    "            self.net = tv.resnet152(pretrained=pretrained)\n",
    "        self.N_slices = 5\n",
    "\n",
    "        self.conv1 = self.net.conv1\n",
    "        self.bn1 = self.net.bn1\n",
    "        self.relu = self.net.relu\n",
    "        self.maxpool = self.net.maxpool\n",
    "        self.layer1 = self.net.layer1\n",
    "        self.layer2 = self.net.layer2\n",
    "        self.layer3 = self.net.layer3\n",
    "        self.layer4 = self.net.layer4\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.conv1(X)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        h_relu1 = h\n",
    "        h = self.maxpool(h)\n",
    "        h = self.layer1(h)\n",
    "        h_conv2 = h\n",
    "        h = self.layer2(h)\n",
    "        h_conv3 = h\n",
    "        h = self.layer3(h)\n",
    "        h_conv4 = h\n",
    "        h = self.layer4(h)\n",
    "        h_conv5 = h\n",
    "\n",
    "        outputs = namedtuple(\"Outputs\", ['relu1','conv2','conv3','conv4','conv5'])\n",
    "        out = outputs(h_relu1, h_conv2, h_conv3, h_conv4, h_conv5)\n",
    "\n",
    "        return out\n",
    "\n",
    "def upsample(in_tens, out_HW=(64,64)): # assumes scale factor is same for H and W\n",
    "    in_H, in_W = in_tens.shape[2], in_tens.shape[3]\n",
    "    return nn.Upsample(size=out_HW, mode='bilinear', align_corners=False)(in_tens)\n",
    "\n",
    "# Learned perceptual metric\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, pretrained=True, net='squeeze', version='0.1', lpips=True, spatial=False,\n",
    "        pnet_rand=False, pnet_tune=False, use_dropout=True, model_path=None, eval_mode=True, verbose=True):\n",
    "        \"\"\" Initializes a perceptual loss torch.nn.Module\n",
    "\n",
    "        Parameters (default listed first)\n",
    "        ---------------------------------\n",
    "        lpips : bool\n",
    "            [True] use linear layers on top of base/trunk network\n",
    "            [False] means no linear layers; each layer is averaged together\n",
    "        pretrained : bool\n",
    "            This flag controls the linear layers, which are only in effect when lpips=True above\n",
    "            [True] means linear layers are calibrated with human perceptual judgments\n",
    "            [False] means linear layers are randomly initialized\n",
    "        pnet_rand : bool\n",
    "            [False] means trunk loaded with ImageNet classification weights\n",
    "            [True] means randomly initialized trunk\n",
    "        net : str\n",
    "            ['alex','vgg','squeeze'] are the base/trunk networks available\n",
    "        version : str\n",
    "            ['v0.1'] is the default and latest\n",
    "            ['v0.0'] contained a normalization bug; corresponds to old arxiv v1 (https://arxiv.org/abs/1801.03924v1)\n",
    "        model_path : 'str'\n",
    "            [None] is default and loads the pretrained weights from paper https://arxiv.org/abs/1801.03924v1\n",
    "\n",
    "        The following parameters should only be changed if training the network\n",
    "\n",
    "        eval_mode : bool\n",
    "            [True] is for test mode (default)\n",
    "            [False] is for training mode\n",
    "        pnet_tune\n",
    "            [False] keep base/trunk frozen\n",
    "            [True] tune the base/trunk network\n",
    "        use_dropout : bool\n",
    "            [True] to use dropout when training linear layers\n",
    "            [False] for no dropout when training linear layers\n",
    "        \"\"\"\n",
    "\n",
    "        super(LPIPS, self).__init__()\n",
    "        if(verbose):\n",
    "            print('Setting up [%s] perceptual loss: trunk [%s], v[%s], spatial [%s]'%\n",
    "                ('LPIPS' if lpips else 'baseline', net, version, 'on' if spatial else 'off'))\n",
    "\n",
    "        self.pnet_type = net\n",
    "        self.pnet_tune = pnet_tune\n",
    "        self.pnet_rand = pnet_rand\n",
    "        self.spatial = spatial\n",
    "        self.lpips = lpips # false means baseline of just averaging all layers\n",
    "        self.version = version\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "\n",
    "        if(self.pnet_type in ['vgg','vgg16']):\n",
    "            net_type = vgg16\n",
    "            self.chns = [64,128,256,512,512]\n",
    "        elif(self.pnet_type=='alex'):\n",
    "            net_type = alexnet\n",
    "            self.chns = [64,192,384,256,256]\n",
    "        elif(self.pnet_type=='squeeze'):\n",
    "            net_type = squeezenet\n",
    "            self.chns = [64,128,256,384,384,512,512]\n",
    "        self.L = len(self.chns)\n",
    "\n",
    "        self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)\n",
    "\n",
    "        if(lpips):\n",
    "            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "            self.lins = [self.lin0,self.lin1,self.lin2,self.lin3,self.lin4]\n",
    "            if(self.pnet_type=='squeeze'): # 7 layers for squeezenet\n",
    "                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)\n",
    "                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)\n",
    "                self.lins+=[self.lin5,self.lin6]\n",
    "            self.lins = nn.ModuleList(self.lins)\n",
    "\n",
    "            if(pretrained):\n",
    "                if(model_path is None):\n",
    "                    # import inspect\n",
    "                    # import os\n",
    "                    # model_path = os.path.abspath(os.path.join(inspect.getfile(self.__init__), '..', 'weights/v%s/%s.pth'%(version,net)))\n",
    "                    model_path = os.path.abspath(os.path.join('./', 'weights/v%s/%s.pth'%(version,net)))\n",
    "\n",
    "                if(verbose):\n",
    "                    print('Loading model from: %s'%model_path)\n",
    "                self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "\n",
    "        if(eval_mode):\n",
    "            self.eval()\n",
    "\n",
    "    def forward(self, in0, in1, retPerLayer=False, normalize=False, mask=None):\n",
    "        return lpips_forward(self, in0, in1, retPerLayer, normalize, mask)\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer('shift', torch.Tensor([-.030,-.088,-.188])[None,:,None,None])\n",
    "        self.register_buffer('scale', torch.Tensor([.458,.448,.450])[None,:,None,None])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    ''' A single linear layer which does a 1x1 conv '''\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "\n",
    "        layers = [nn.Dropout(),] if(use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Dist2LogitLayer(nn.Module):\n",
    "    ''' takes 2 distances, puts through fc layers, spits out value between [0,1] (if use_sigmoid is True) '''\n",
    "    def __init__(self, chn_mid=32, use_sigmoid=True):\n",
    "        super(Dist2LogitLayer, self).__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True),]\n",
    "        layers += [nn.LeakyReLU(0.2,True),]\n",
    "        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1, padding=0, bias=True),]\n",
    "        layers += [nn.LeakyReLU(0.2,True),]\n",
    "        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True),]\n",
    "        if(use_sigmoid):\n",
    "            layers += [nn.Sigmoid(),]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,d0,d1,eps=0.1):\n",
    "        return self.model.forward(torch.cat((d0,d1,d0-d1,d0/(d1+eps),d1/(d0+eps)),dim=1))\n",
    "\n",
    "class BCERankingLoss(nn.Module):\n",
    "    def __init__(self, chn_mid=32):\n",
    "        super(BCERankingLoss, self).__init__()\n",
    "        self.net = Dist2LogitLayer(chn_mid=chn_mid)\n",
    "        # self.parameters = list(self.net.parameters())\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, d0, d1, judge):\n",
    "        per = (judge+1.)/2.\n",
    "        self.logit = self.net.forward(d0,d1)\n",
    "        return self.loss(self.logit, per)\n",
    "\n",
    "\n",
    "def spatial_average(in_tens, keepdim=True, mask=None):\n",
    "    if mask is not None:\n",
    "        resizing = Resize((in_tens.shape[2], in_tens.shape[3]))\n",
    "        resized_mask = resizing(mask)\n",
    "\n",
    "        # fig, ax = plt.subplots(1, 3)\n",
    "        # fig.set_figwidth(15)\n",
    "        # fig.set_figheight(7)\n",
    "\n",
    "        # img = ax[0].imshow(in_tens[0, 0].cpu().detach())\n",
    "        # fig.colorbar(img)\n",
    "        # img = ax[1].imshow(resized_mask[0, 0].cpu().detach())\n",
    "        # fig.colorbar(img)\n",
    "\n",
    "        in_tens = resized_mask * in_tens\n",
    "        # img = ax[2].imshow(in_tens[0, 0].cpu().detach())\n",
    "        # fig.colorbar(img)\n",
    "        # fig.show()\n",
    "        return torch.sum(in_tens, dim=(2, 3), keepdim=keepdim) / torch.sum(resized_mask, dim=(2, 3))\n",
    "\n",
    "    return in_tens.mean([2,3],keepdim=keepdim)\n",
    "\n",
    "def lpips_forward(self, in0, in1, retPerLayer=False, normalize=False, mask=None):\n",
    "    if normalize: # turn on this flag if input is [0,1] so it can be adjusted to [-1, +1]\n",
    "        in0 = 2 * in0  - 1\n",
    "        in1 = 2 * in1  - 1\n",
    "\n",
    "    # v0.0 - original release had a bug, where input was not scaled\n",
    "    in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version=='0.1' else (in0, in1)\n",
    "    outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "    feats0, feats1, diffs = {}, {}, {}\n",
    "\n",
    "    for kk in range(self.L):\n",
    "        feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n",
    "        diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "    if(self.lpips):\n",
    "        if(self.spatial):\n",
    "            res = [upsample(self.lins[kk](diffs[kk]), out_HW=in0.shape[2:]) for kk in range(self.L)]\n",
    "        else:\n",
    "            res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True, mask=mask) for kk in range(self.L)]\n",
    "    else:\n",
    "        if(self.spatial):\n",
    "            res = [upsample(diffs[kk].sum(dim=1,keepdim=True), out_HW=in0.shape[2:]) for kk in range(self.L)]\n",
    "        else:\n",
    "            res = [spatial_average(diffs[kk].sum(dim=1,keepdim=True), keepdim=True, mask=mask) for kk in range(self.L)]\n",
    "\n",
    "    val = 0\n",
    "    for l in range(self.L):\n",
    "        val += res[l]\n",
    "\n",
    "    if(retPerLayer):\n",
    "        return (val, res)\n",
    "    else:\n",
    "        return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MS-SSIM\n",
    "\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def _fspecial_gauss_1d(size: int, sigma: float) -> Tensor:\n",
    "    r\"\"\"Create 1-D gauss kernel\n",
    "    Args:\n",
    "        size (int): the size of gauss kernel\n",
    "        sigma (float): sigma of normal distribution\n",
    "    Returns:\n",
    "        torch.Tensor: 1D kernel (1 x 1 x size)\n",
    "    \"\"\"\n",
    "    coords = torch.arange(size, dtype=torch.float)\n",
    "    coords -= size // 2\n",
    "\n",
    "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "    g /= g.sum()\n",
    "\n",
    "    return g.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def gaussian_filter(input: Tensor, win: Tensor) -> Tensor:\n",
    "    r\"\"\" Blur input with 1-D kernel\n",
    "    Args:\n",
    "        input (torch.Tensor): a batch of tensors to be blurred\n",
    "        window (torch.Tensor): 1-D gauss kernel\n",
    "    Returns:\n",
    "        torch.Tensor: blurred tensors\n",
    "    \"\"\"\n",
    "    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n",
    "    if len(input.shape) == 4:\n",
    "        conv = F.conv2d\n",
    "    elif len(input.shape) == 5:\n",
    "        conv = F.conv3d\n",
    "    else:\n",
    "        raise NotImplementedError(input.shape)\n",
    "\n",
    "    C = input.shape[1]\n",
    "    out = input\n",
    "    for i, s in enumerate(input.shape[2:]):\n",
    "        if s >= win.shape[-1]:\n",
    "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float,\n",
    "    win: Tensor,\n",
    "    size_average: bool = True,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    mask: Tensor = None\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\" Calculate ssim index for X and Y\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): images\n",
    "        Y (torch.Tensor): images\n",
    "        data_range (float or int): value range of input images. (usually 1.0 or 255)\n",
    "        win (torch.Tensor): 1-D gauss kernel\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: ssim results.\n",
    "    \"\"\"\n",
    "    K1, K2 = K\n",
    "    # batch, channel, [depth,] height, width = X.shape\n",
    "    compensation = 1.0\n",
    "\n",
    "    C1 = (K1 * data_range) ** 2\n",
    "    C2 = (K2 * data_range) ** 2\n",
    "\n",
    "    win = win.to(X.device, dtype=X.dtype)\n",
    "\n",
    "    mu1 = gaussian_filter(X, win)\n",
    "    mu2 = gaussian_filter(Y, win)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n",
    "    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n",
    "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n",
    "\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n",
    "\n",
    "    if mask is not None:\n",
    "        resizing = Resize(cs_map.shape[-2:])\n",
    "        weights = resizing(mask).to(device).float()\n",
    "        cs_map = 2 * (((cs_map + 1) / 2) ** weights) - 1\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "\n",
    "    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n",
    "    cs = torch.flatten(cs_map, 2).mean(-1)\n",
    "    return ssim_per_channel, cs\n",
    "\n",
    "\n",
    "def ms_ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float = 255,\n",
    "    size_average: bool = True,\n",
    "    win_size: int = 11,\n",
    "    win_sigma: float = 1.5,\n",
    "    win: Optional[Tensor] = None,\n",
    "    weights: Optional[List[float]] = None,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    mask: Tensor = None\n",
    ") -> Tensor:\n",
    "    r\"\"\" interface of ms-ssim\n",
    "    Args:\n",
    "        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        weights (list, optional): weights for different levels\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "    Returns:\n",
    "        torch.Tensor: ms-ssim results\n",
    "    \"\"\"\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
    "\n",
    "    for d in range(len(X.shape) - 1, 1, -1):\n",
    "        X = X.squeeze(dim=d)\n",
    "        Y = Y.squeeze(dim=d)\n",
    "\n",
    "    #if not X.type() == Y.type():\n",
    "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
    "\n",
    "    if len(X.shape) == 4:\n",
    "        avg_pool = F.avg_pool2d\n",
    "    elif len(X.shape) == 5:\n",
    "        avg_pool = F.avg_pool3d\n",
    "    else:\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
    "\n",
    "    if win is not None:  # set win_size\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError(\"Window size should be odd.\")\n",
    "\n",
    "    smaller_side = min(X.shape[-2:])\n",
    "    assert smaller_side > (win_size - 1) * (\n",
    "        2 ** 4\n",
    "    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "    weights_tensor = X.new_tensor(weights)\n",
    "\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
    "\n",
    "    levels = weights_tensor.shape[0]\n",
    "    mcs = []\n",
    "    for i in range(levels):\n",
    "        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K, mask=mask)\n",
    "        mask = None\n",
    "\n",
    "        if i < levels - 1:\n",
    "            mcs.append(torch.relu(cs))\n",
    "            padding = [s % 2 for s in X.shape[2:]]\n",
    "            X = avg_pool(X, kernel_size=2, padding=padding)\n",
    "            Y = avg_pool(Y, kernel_size=2, padding=padding)\n",
    "\n",
    "    ssim_per_channel = torch.relu(ssim_per_channel)  # type: ignore  # (batch, channel)\n",
    "    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n",
    "    ms_ssim_val = torch.prod(mcs_and_ssim ** weights_tensor.view(-1, 1, 1), dim=0)\n",
    "\n",
    "    if size_average:\n",
    "        return ms_ssim_val.mean()\n",
    "    else:\n",
    "        return ms_ssim_val.mean(1)\n",
    "\n",
    "class MS_SSIM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range: float = 255,\n",
    "        size_average: bool = True,\n",
    "        win_size: int = 11,\n",
    "        win_sigma: float = 1.5,\n",
    "        channel: int = 3,\n",
    "        spatial_dims: int = 2,\n",
    "        weights: Optional[List[float]] = None,\n",
    "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    ) -> None:\n",
    "        r\"\"\" class for ms-ssim\n",
    "        Args:\n",
    "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "            win_size: (int, optional): the size of gauss kernel\n",
    "            win_sigma: (float, optional): sigma of normal distribution\n",
    "            channel (int, optional): input channels (default: 3)\n",
    "            weights (list, optional): weights for different levels\n",
    "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MS_SSIM, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.weights = weights\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, X: Tensor, Y: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        return ms_ssim(\n",
    "            X,\n",
    "            Y,\n",
    "            data_range=self.data_range,\n",
    "            size_average=self.size_average,\n",
    "            win=self.win,\n",
    "            weights=self.weights,\n",
    "            K=self.K,\n",
    "            mask=mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips_alex = LPIPS(net='alex').to(device)\n",
    "# lpips_squeeze = LPIPS(net='squeeze').to(device)\n",
    "# lpips_vgg = LPIPS(net='vgg').to(device)\n",
    "\n",
    "lpips_alex_loss = lambda x, y: lpips_alex(2 * x - 1, 2 * y - 1)\n",
    "# lpips_squeeze_loss = lambda x, y: lpips_squeeze(2 * x - 1, 2 * y - 1)\n",
    "# lpips_vgg_loss = lambda x, y: lpips_vgg(2 * x - 1, 2 * y - 1)\n",
    "\n",
    "msssim = MS_SSIM(data_range=1, size_average=True, channel=3)\n",
    "l2 = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "    \n",
    "def evaluate_loss(model: nn.Module, dataloader, amount: int=-1, is_two_autoencoder: bool = True) -> None:\n",
    "    loss_l2 = 0\n",
    "    loss_l1 = 0\n",
    "    loss_alex = 0\n",
    "    loss_squeeze = 0\n",
    "    loss_vgg = 0\n",
    "    loss_msssim = 0\n",
    "    loss_psnr = 0\n",
    "\n",
    "    for i, (imgs, faces, bboxes) in enumerate(tqdm(dataloader)):\n",
    "        if i == amount:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if is_two_autoencoder:\n",
    "                imgs, faces = imgs.to(device), faces.to(device)\n",
    "                \n",
    "                fg_masks = create_fg_masks(bboxes).to(device)\n",
    "                fg_output, bg_output = model(imgs * (~fg_masks), faces)\n",
    "                \n",
    "                imgs_reconstruction = stitch(fg_output, bg_output,\n",
    "                                             bboxes.to(device), feather_size=20, device=device)\n",
    "            else:\n",
    "                imgs = imgs.to(device)\n",
    "                \n",
    "                imgs_reconstruction = model(imgs)\n",
    "            \n",
    "            loss_l2 += l2(imgs, imgs_reconstruction)\n",
    "            loss_l1 += l1(imgs, imgs_reconstruction)\n",
    "            loss_alex += lpips_alex_loss(imgs, imgs_reconstruction).sum()\n",
    "            # loss_squeeze += lpips_squeeze_loss(imgs, imgs_reconstruction)\n",
    "            # loss_vgg += lpips_vgg_loss(imgs, imgs_reconstruction)\n",
    "            loss_msssim += msssim(imgs, imgs_reconstruction)\n",
    "            loss_psnr += psnr(imgs, imgs_reconstruction, peak=1)\n",
    "\n",
    "        del imgs, imgs_reconstruction\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    n = max(len(dataloader), amount)\n",
    "    process = lambda x: round(x.item() / n, 5)\n",
    "\n",
    "    loss_l2 = process(loss_l2)\n",
    "    loss_l1 = process(loss_l1)\n",
    "    loss_alex = process(loss_alex)\n",
    "    # loss_squeeze = process(torch.mean(loss_squeeze))\n",
    "    # loss_vgg = process(torch.mean(loss_vgg))\n",
    "    loss_msssim = process(loss_msssim)\n",
    "    loss_psnr = process(loss_psnr)\n",
    "\n",
    "    print(f\"MODEL EVALUATION\")\n",
    "    print(f\"----------------\")\n",
    "    print(f\"{loss_l2}\\tL2\")\n",
    "    print(f\"{loss_l1}\\tL1\")\n",
    "    print(f\"{loss_alex}\\tLPIPS-Alex\")\n",
    "    # print(f\"{loss_squeeze}\\tLPIPS-Squeeze\")\n",
    "    # print(f\"{loss_vgg}\\tLPIPS-VGG\")\n",
    "    print(f\"{loss_msssim}\\tMS-SSIM\")\n",
    "    print(f\"{loss_psnr}\\tPSNR\")\n",
    "    \n",
    "    \n",
    "def evaluate_loss_no_model(dataloader, amount: int=-1) -> None:\n",
    "    loss_l2 = 0\n",
    "    loss_l1 = 0\n",
    "    loss_alex = 0\n",
    "    # loss_squeeze = 0\n",
    "    # loss_vgg = 0\n",
    "    loss_msssim = 0\n",
    "    loss_psnr = 0\n",
    "\n",
    "    for i, (imgs, imgs_reconstruction) in enumerate(tqdm(dataloader)):        \n",
    "        if i == amount:\n",
    "            break\n",
    "\n",
    "        imgs, imgs_reconstruction = imgs.to(device), imgs_reconstruction.to(device)\n",
    "        \n",
    "        loss_l2 += l2(imgs, imgs_reconstruction)\n",
    "        loss_l1 += l1(imgs, imgs_reconstruction)\n",
    "        loss_alex += lpips_alex_loss(imgs, imgs_reconstruction).sum()\n",
    "        # loss_squeeze += lpips_squeeze_loss(imgs, imgs_reconstruction)\n",
    "        # loss_vgg += lpips_vgg_loss(imgs, imgs_reconstruction)\n",
    "        loss_msssim += msssim(imgs, imgs_reconstruction)\n",
    "        loss_psnr += psnr(imgs, imgs_reconstruction, peak=1)\n",
    "\n",
    "        del imgs, imgs_reconstruction\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    n = max(len(dataloader), amount)\n",
    "    process = lambda x: round(x.item() / n, 5)\n",
    "\n",
    "    loss_l2 = process(loss_l2)\n",
    "    loss_l1 = process(loss_l1)\n",
    "    loss_alex = process(loss_alex)\n",
    "    # loss_squeeze = process(torch.mean(loss_squeeze))\n",
    "    # loss_vgg = process(torch.mean(loss_vgg))\n",
    "    loss_msssim = process(loss_msssim)\n",
    "    loss_psnr = process(loss_psnr)\n",
    "\n",
    "    print(f\"MODEL EVALUATION\")\n",
    "    print(f\"----------------\")\n",
    "    print(f\"{loss_l2}\\tL2\")\n",
    "    print(f\"{loss_l1}\\tL1\")\n",
    "    print(f\"{loss_alex}\\tLPIPS-Alex\")\n",
    "    # print(f\"{loss_squeeze}\\tLPIPS-Squeeze\")\n",
    "    # print(f\"{loss_vgg}\\tLPIPS-VGG\")\n",
    "    print(f\"{loss_msssim}\\tMS-SSIM\")\n",
    "    print(f\"{loss_psnr}\\tPSNR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate quantized model\n",
    "model = torch.load('boiNet750,250.pt')\n",
    "quant = QuantizedTwoResAutoEncoder(model)\n",
    "quant.to(device)\n",
    "\n",
    "quant.train()\n",
    "\n",
    "# Find optimal training sizes\n",
    "dataset = FaceDataset('test_set_bb.csv', '../celeba/img_align_celeba', return_im_num=False)\n",
    "dataloader = DataLoader(dataset, batch_size=1024)\n",
    "\n",
    "quant.to('cpu')\n",
    "\n",
    "for images, faces, bboxs in dataloader:\n",
    "    with torch.no_grad():\n",
    "        images, faces = images.to(device), faces.to(device)\n",
    "    \n",
    "        fg_masks = create_fg_masks(bboxs).to(device)\n",
    "        fg_output, bg_output = quant(images * (~fg_masks), faces)\n",
    "        \n",
    "        \n",
    "print(quant.fg_ae.h_M, quant.bg_ae.h_M)\n",
    "print(quant.fg_ae.h_M.max(), quant.bg_ae.h_M.max())\n",
    "print((quant.fg_ae.h_M == 0).sum() / quant.fg_ae.h_M.numel(), (quant.bg_ae.h_M == 0).sum() / quant.fg_ae.h_M.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant.eval()\n",
    "\n",
    "evaluate_loss(quant, dataloader, is_two_autoencoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on their own\n",
    "\n",
    "model_filenames = [filename for filename in os.listdir('.') if filename.endswith('.pt')]\n",
    "\n",
    "dataset = FaceDataset('test_set_bb.csv', '../celeba/img_align_celeba', return_im_num=False)\n",
    "dataloader = DataLoader(dataset, batch_size=1024)\n",
    "\n",
    "for filename in model_filenames:\n",
    "    print(filename)\n",
    "    model = torch.load(filename)\n",
    "    model.to(device)\n",
    "\n",
    "    evaluate_loss(model, dataloader, is_two_autoencoder=not 'Base' in filename)\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dir = '../celeba/img_align_celeba' \n",
    "after_dir = '../compressed_jpg'\n",
    "\n",
    "dataset = JpgBeforeAfterDataset(before_dir, after_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "evaluate_loss_no_model(dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
