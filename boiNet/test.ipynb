{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import FaceDataset \n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = 'norm_bounding_boxes.csv'\n",
    "# Initialize dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, df_path, image_dir):\n",
    "        self.dataframe = pd.read_csv(df_path)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        # Transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((92, 84)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        bbox = self.dataframe.iloc[idx, 1:5].values\n",
    "        face_crop = image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            face_crop = self.transform(face_crop)\n",
    "\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        image = to_tensor(image)\n",
    "        \n",
    "        bbox = np.array(bbox, dtype=int)\n",
    "        \n",
    "        return image, face_crop, torch.tensor(bbox)\n",
    "    \n",
    "dataset = FaceDataset(df_path, '../celeba/img_align_celeba')\n",
    "dataloader = DataLoader(dataset, batch_size=25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3706 8414  415 5673 1754 3911 4036 7745  578 9753 6454 5737 8349 2997\n",
      "  587 9785 4349 9435  809 8517 9639 8372 5475 7011 9527 1788 3220 5148\n",
      " 5644 1157 8917 1796 7851 3555 7375 4327 3705 7945 6212 3548 3051 5686\n",
      " 7364 2870 8302 1328 3124 8519 2319 4754 1670  384 3757 3468 8489 7740\n",
      " 5375 5586 9367 5940 8671 9359 1822 1154 4175 8141 9340  377 1981 2009\n",
      " 2564 6486 4303 8169 5641 9163 8086 8478 6582 9224  834 3754  389 8938\n",
      "  411 1773 6050  433 1425 1388 8996 1138 3251 1011 2215 5010 8048 5254\n",
      " 8287 7217]\n",
      "[tensor([[[[0.9843, 0.9882, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9843, 0.9882, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9843, 0.9882, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.8196, 0.7725, 0.7529,  ..., 0.7059, 0.7098, 0.7137],\n",
      "          [0.8588, 0.7804, 0.7451,  ..., 0.7098, 0.7059, 0.7059],\n",
      "          [0.8588, 0.7804, 0.7451,  ..., 0.7020, 0.7059, 0.7059]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.4902, 0.4275, 0.3882,  ..., 0.5490, 0.5647, 0.5686],\n",
      "          [0.5294, 0.4353, 0.3804,  ..., 0.5529, 0.5608, 0.5608],\n",
      "          [0.5294, 0.4353, 0.3804,  ..., 0.5529, 0.5608, 0.5608]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.7961, 0.7569, 0.7451,  ..., 0.5922, 0.5922, 0.5961],\n",
      "          [0.8353, 0.7647, 0.7373,  ..., 0.5961, 0.5882, 0.5882],\n",
      "          [0.8353, 0.7647, 0.7373,  ..., 0.5961, 0.5882, 0.5882]]],\n",
      "\n",
      "\n",
      "        [[[0.6980, 0.7020, 0.7098,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.6196, 0.6275, 0.6392,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.6235, 0.6235, 0.6314,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.1569, 0.0902, 0.0824,  ..., 0.4784, 0.4118, 0.3804],\n",
      "          [0.0627, 0.0824, 0.0235,  ..., 0.4745, 0.4196, 0.3451],\n",
      "          [0.0353, 0.0549, 0.0000,  ..., 0.4745, 0.4157, 0.3412]],\n",
      "\n",
      "         [[0.7333, 0.7373, 0.7451,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.6549, 0.6627, 0.6745,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.6510, 0.6510, 0.6588,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.3373, 0.2588, 0.2392,  ..., 0.4784, 0.4353, 0.4039],\n",
      "          [0.3529, 0.3529, 0.2549,  ..., 0.4745, 0.4431, 0.3686],\n",
      "          [0.3725, 0.3686, 0.2706,  ..., 0.4745, 0.4471, 0.3725]],\n",
      "\n",
      "         [[0.4706, 0.4745, 0.4824,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.4078, 0.4157, 0.4275,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.4392, 0.4392, 0.4471,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.3961, 0.3333, 0.3373,  ..., 0.4706, 0.3882, 0.3569],\n",
      "          [0.3490, 0.3725, 0.3098,  ..., 0.4667, 0.4039, 0.3294],\n",
      "          [0.3373, 0.3569, 0.2980,  ..., 0.4667, 0.4039, 0.3294]]],\n",
      "\n",
      "\n",
      "        [[[0.8510, 0.8510, 0.8510,  ..., 0.7882, 0.8902, 0.8902],\n",
      "          [0.8510, 0.8510, 0.8510,  ..., 0.7961, 0.8824, 0.8824],\n",
      "          [0.8510, 0.8510, 0.8510,  ..., 0.8118, 0.8745, 0.8745],\n",
      "          ...,\n",
      "          [0.8627, 0.8627, 0.8667,  ..., 0.1216, 0.1255, 0.1255],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.1294, 0.1294, 0.1294],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.1294, 0.1294, 0.1294]],\n",
      "\n",
      "         [[0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8588, 0.8588],\n",
      "          [0.8431, 0.8431, 0.8431,  ..., 0.8784, 0.8510, 0.8510],\n",
      "          [0.8431, 0.8431, 0.8431,  ..., 0.8784, 0.8392, 0.8392],\n",
      "          ...,\n",
      "          [0.8627, 0.8627, 0.8667,  ..., 0.1216, 0.1255, 0.1255],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.1294, 0.1294, 0.1294],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.1294, 0.1294, 0.1294]],\n",
      "\n",
      "         [[0.8549, 0.8549, 0.8549,  ..., 0.9569, 0.8510, 0.8510],\n",
      "          [0.8549, 0.8549, 0.8549,  ..., 0.9529, 0.8431, 0.8431],\n",
      "          [0.8549, 0.8549, 0.8549,  ..., 0.9412, 0.8431, 0.8431],\n",
      "          ...,\n",
      "          [0.8627, 0.8627, 0.8667,  ..., 0.1216, 0.1255, 0.1255],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.1294, 0.1294, 0.1294],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.1294, 0.1294, 0.1294]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8941, 0.8941],\n",
      "          [0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8902, 0.8902],\n",
      "          [0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8863, 0.8863],\n",
      "          ...,\n",
      "          [0.3922, 0.4078, 0.4314,  ..., 0.5255, 0.6039, 0.5804],\n",
      "          [0.3608, 0.4471, 0.4745,  ..., 0.5216, 0.5725, 0.5412],\n",
      "          [0.3608, 0.4471, 0.4706,  ..., 0.5333, 0.5529, 0.5255]],\n",
      "\n",
      "         [[0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8941, 0.8941],\n",
      "          [0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8902, 0.8902],\n",
      "          [0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8863, 0.8863],\n",
      "          ...,\n",
      "          [0.2745, 0.2863, 0.2980,  ..., 0.3843, 0.4784, 0.4549],\n",
      "          [0.2431, 0.3255, 0.3412,  ..., 0.3804, 0.4471, 0.4157],\n",
      "          [0.2471, 0.3255, 0.3373,  ..., 0.3922, 0.4275, 0.4000]],\n",
      "\n",
      "         [[0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8941, 0.8941],\n",
      "          [0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8902, 0.8902],\n",
      "          [0.8235, 0.8275, 0.8314,  ..., 0.8980, 0.8863, 0.8863],\n",
      "          ...,\n",
      "          [0.1647, 0.1765, 0.2000,  ..., 0.2980, 0.4196, 0.3961],\n",
      "          [0.1333, 0.2157, 0.2353,  ..., 0.2863, 0.3961, 0.3647],\n",
      "          [0.1294, 0.2157, 0.2314,  ..., 0.2902, 0.3765, 0.3490]]],\n",
      "\n",
      "\n",
      "        [[[0.9255, 0.9098, 0.9255,  ..., 0.5804, 0.5922, 0.5922],\n",
      "          [0.9255, 0.9098, 0.9255,  ..., 0.5882, 0.5922, 0.5922],\n",
      "          [0.9255, 0.9098, 0.9255,  ..., 0.5843, 0.5843, 0.5843],\n",
      "          ...,\n",
      "          [0.8314, 0.8314, 0.8314,  ..., 0.0235, 0.0235, 0.0235],\n",
      "          [0.8471, 0.8471, 0.8471,  ..., 0.0235, 0.0157, 0.0157],\n",
      "          [0.8471, 0.8471, 0.8471,  ..., 0.0235, 0.0157, 0.0157]],\n",
      "\n",
      "         [[0.8275, 0.8118, 0.8235,  ..., 0.7569, 0.7686, 0.7686],\n",
      "          [0.8275, 0.8118, 0.8235,  ..., 0.7647, 0.7686, 0.7686],\n",
      "          [0.8275, 0.8118, 0.8235,  ..., 0.7608, 0.7608, 0.7608],\n",
      "          ...,\n",
      "          [0.9333, 0.9333, 0.9333,  ..., 0.1255, 0.1255, 0.1255],\n",
      "          [0.9490, 0.9490, 0.9490,  ..., 0.1255, 0.1294, 0.1294],\n",
      "          [0.9490, 0.9490, 0.9490,  ..., 0.1255, 0.1294, 0.1294]],\n",
      "\n",
      "         [[0.6039, 0.5922, 0.6314,  ..., 0.7294, 0.7412, 0.7412],\n",
      "          [0.6039, 0.5922, 0.6314,  ..., 0.7373, 0.7412, 0.7412],\n",
      "          [0.6039, 0.5922, 0.6314,  ..., 0.7333, 0.7333, 0.7333],\n",
      "          ...,\n",
      "          [0.9373, 0.9373, 0.9373,  ..., 0.1294, 0.1216, 0.1216],\n",
      "          [0.9529, 0.9529, 0.9529,  ..., 0.1216, 0.1137, 0.1137],\n",
      "          [0.9529, 0.9529, 0.9529,  ..., 0.1216, 0.1137, 0.1137]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.5647, 0.4824, 0.5059,  ..., 0.5765, 0.6941, 0.6941],\n",
      "          [0.5804, 0.5373, 0.5176,  ..., 0.6000, 0.5922, 0.6000],\n",
      "          [0.6118, 0.5725, 0.5490,  ..., 0.5922, 0.5765, 0.5843]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.3412, 0.2667, 0.2941,  ..., 0.3608, 0.5216, 0.5216],\n",
      "          [0.3451, 0.3176, 0.3020,  ..., 0.3725, 0.4235, 0.4314],\n",
      "          [0.3804, 0.3412, 0.3216,  ..., 0.3647, 0.4078, 0.4157]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.2275, 0.1451, 0.1451,  ..., 0.2000, 0.3294, 0.3294],\n",
      "          [0.2353, 0.1882, 0.1490,  ..., 0.2235, 0.2235, 0.2314],\n",
      "          [0.2627, 0.2078, 0.1647,  ..., 0.2157, 0.2078, 0.2157]]]]), tensor([[[[0.1843, 0.1843, 0.1961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.1922, 0.1922, 0.2000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.2000, 0.1961, 0.2000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.8510, 0.7804, 0.7529,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.8510, 0.7961, 0.7961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.8275, 0.8196, 0.8980,  ..., 1.0000, 0.9961, 0.9922]],\n",
      "\n",
      "         [[0.0706, 0.0706, 0.0706,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0824, 0.0784, 0.0784,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.0941, 0.0824, 0.0784,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.3725, 0.2824, 0.2314,  ..., 0.9882, 0.9882, 0.9882],\n",
      "          [0.3765, 0.3059, 0.2784,  ..., 0.9882, 0.9882, 0.9922],\n",
      "          [0.3529, 0.3373, 0.3922,  ..., 0.9882, 0.9804, 0.9765]],\n",
      "\n",
      "         [[0.1569, 0.1490, 0.1529,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.1686, 0.1569, 0.1490,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.1686, 0.1608, 0.1490,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.8078, 0.7333, 0.6902,  ..., 0.9922, 0.9922, 0.9922],\n",
      "          [0.8196, 0.7647, 0.7451,  ..., 0.9922, 0.9922, 0.9843],\n",
      "          [0.8039, 0.7922, 0.8588,  ..., 0.9843, 0.9765, 0.9725]]],\n",
      "\n",
      "\n",
      "        [[[0.2941, 0.2588, 0.2431,  ..., 0.9137, 0.7765, 0.6627],\n",
      "          [0.2510, 0.2392, 0.2118,  ..., 0.6157, 0.6745, 0.7020],\n",
      "          [0.2118, 0.2275, 0.1843,  ..., 0.6275, 0.7137, 0.7804],\n",
      "          ...,\n",
      "          [0.1490, 0.0863, 0.0667,  ..., 0.1294, 0.1529, 0.1569],\n",
      "          [0.1098, 0.0510, 0.0667,  ..., 0.2157, 0.1961, 0.2039],\n",
      "          [0.0863, 0.0431, 0.0588,  ..., 0.2863, 0.1922, 0.2039]],\n",
      "\n",
      "         [[0.2902, 0.2627, 0.2510,  ..., 0.6510, 0.5216, 0.4275],\n",
      "          [0.2471, 0.2431, 0.2196,  ..., 0.2549, 0.2941, 0.3098],\n",
      "          [0.2078, 0.2314, 0.2000,  ..., 0.1961, 0.2157, 0.2392],\n",
      "          ...,\n",
      "          [0.2549, 0.2078, 0.2039,  ..., 0.2275, 0.2431, 0.2471],\n",
      "          [0.2118, 0.1686, 0.2039,  ..., 0.2275, 0.1922, 0.2078],\n",
      "          [0.1804, 0.1490, 0.1843,  ..., 0.2745, 0.1529, 0.1765]],\n",
      "\n",
      "         [[0.3098, 0.2706, 0.2471,  ..., 0.6784, 0.5529, 0.4588],\n",
      "          [0.2667, 0.2510, 0.2157,  ..., 0.2745, 0.3294, 0.3569],\n",
      "          [0.2275, 0.2392, 0.1961,  ..., 0.2078, 0.2588, 0.2980],\n",
      "          ...,\n",
      "          [0.3216, 0.2863, 0.2824,  ..., 0.1451, 0.1412, 0.1333],\n",
      "          [0.3020, 0.2627, 0.2902,  ..., 0.2471, 0.1843, 0.1765],\n",
      "          [0.2824, 0.2549, 0.2824,  ..., 0.3882, 0.2510, 0.2392]]],\n",
      "\n",
      "\n",
      "        [[[0.1608, 0.2549, 0.3765,  ..., 0.2471, 0.1765, 0.1412],\n",
      "          [0.1961, 0.3059, 0.4196,  ..., 0.2667, 0.1843, 0.1451],\n",
      "          [0.2471, 0.3647, 0.4667,  ..., 0.3098, 0.2118, 0.1608],\n",
      "          ...,\n",
      "          [0.5451, 0.5098, 0.5098,  ..., 0.1686, 0.1804, 0.1725],\n",
      "          [0.5255, 0.5412, 0.5647,  ..., 0.1765, 0.1843, 0.1725],\n",
      "          [0.5647, 0.5451, 0.5137,  ..., 0.1765, 0.1804, 0.1647]],\n",
      "\n",
      "         [[0.0941, 0.1686, 0.2667,  ..., 0.1451, 0.1137, 0.1098],\n",
      "          [0.1216, 0.2078, 0.3020,  ..., 0.1608, 0.1176, 0.1059],\n",
      "          [0.1647, 0.2667, 0.3412,  ..., 0.1961, 0.1373, 0.1137],\n",
      "          ...,\n",
      "          [0.7216, 0.6863, 0.6784,  ..., 0.1686, 0.1804, 0.1725],\n",
      "          [0.6902, 0.6980, 0.7098,  ..., 0.1765, 0.1843, 0.1725],\n",
      "          [0.7098, 0.6824, 0.6431,  ..., 0.1765, 0.1804, 0.1647]],\n",
      "\n",
      "         [[0.0235, 0.0863, 0.1804,  ..., 0.0863, 0.0549, 0.0588],\n",
      "          [0.0549, 0.1294, 0.2078,  ..., 0.0941, 0.0549, 0.0588],\n",
      "          [0.0902, 0.1882, 0.2510,  ..., 0.1255, 0.0706, 0.0588],\n",
      "          ...,\n",
      "          [0.8353, 0.8000, 0.8039,  ..., 0.1686, 0.1804, 0.1725],\n",
      "          [0.7843, 0.7961, 0.8118,  ..., 0.1765, 0.1843, 0.1725],\n",
      "          [0.7804, 0.7569, 0.7176,  ..., 0.1765, 0.1804, 0.1647]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6196, 0.6157, 0.6510,  ..., 0.7373, 0.7137, 0.6157],\n",
      "          [0.6275, 0.6392, 0.6510,  ..., 0.7294, 0.7098, 0.6588],\n",
      "          [0.6235, 0.6431, 0.6549,  ..., 0.7412, 0.6941, 0.6549],\n",
      "          ...,\n",
      "          [0.2784, 0.2588, 0.2431,  ..., 0.6157, 0.6627, 0.7686],\n",
      "          [0.2784, 0.2588, 0.2549,  ..., 0.6667, 0.6588, 0.7765],\n",
      "          [0.2706, 0.2510, 0.2784,  ..., 0.7020, 0.6627, 0.7882]],\n",
      "\n",
      "         [[0.4196, 0.4196, 0.4000,  ..., 0.4980, 0.5137, 0.4118],\n",
      "          [0.4196, 0.4314, 0.4000,  ..., 0.4902, 0.5098, 0.4549],\n",
      "          [0.4157, 0.4275, 0.4039,  ..., 0.4941, 0.4941, 0.4510],\n",
      "          ...,\n",
      "          [0.1490, 0.1373, 0.1294,  ..., 0.4784, 0.5176, 0.6235],\n",
      "          [0.1490, 0.1373, 0.1412,  ..., 0.5294, 0.5137, 0.6314],\n",
      "          [0.1412, 0.1294, 0.1647,  ..., 0.5647, 0.5176, 0.6431]],\n",
      "\n",
      "         [[0.2980, 0.2902, 0.2941,  ..., 0.3765, 0.4078, 0.3176],\n",
      "          [0.2980, 0.3059, 0.2941,  ..., 0.3686, 0.4039, 0.3608],\n",
      "          [0.2941, 0.3098, 0.2980,  ..., 0.3725, 0.3882, 0.3569],\n",
      "          ...,\n",
      "          [0.0824, 0.0667, 0.0588,  ..., 0.3686, 0.4118, 0.5176],\n",
      "          [0.0824, 0.0667, 0.0706,  ..., 0.4196, 0.4078, 0.5255],\n",
      "          [0.0745, 0.0588, 0.0941,  ..., 0.4549, 0.4118, 0.5373]]],\n",
      "\n",
      "\n",
      "        [[[0.0706, 0.0902, 0.1373,  ..., 0.5451, 0.5529, 0.5843],\n",
      "          [0.0824, 0.1059, 0.1961,  ..., 0.5490, 0.5529, 0.6275],\n",
      "          [0.1843, 0.2627, 0.3412,  ..., 0.5451, 0.5725, 0.6196],\n",
      "          ...,\n",
      "          [0.4745, 0.5451, 0.6392,  ..., 0.0039, 0.0000, 0.0000],\n",
      "          [0.6510, 0.4706, 0.4706,  ..., 0.0157, 0.0000, 0.0000],\n",
      "          [0.7804, 0.5451, 0.4471,  ..., 0.1020, 0.0667, 0.0431]],\n",
      "\n",
      "         [[0.0902, 0.0863, 0.1059,  ..., 0.5176, 0.5412, 0.5961],\n",
      "          [0.1020, 0.1020, 0.1647,  ..., 0.5216, 0.5529, 0.6392],\n",
      "          [0.1490, 0.1961, 0.2392,  ..., 0.5333, 0.5843, 0.6471],\n",
      "          ...,\n",
      "          [0.4039, 0.4392, 0.4980,  ..., 0.1647, 0.1490, 0.1490],\n",
      "          [0.6000, 0.3961, 0.3569,  ..., 0.1922, 0.1529, 0.1647],\n",
      "          [0.7608, 0.4902, 0.3608,  ..., 0.2902, 0.2588, 0.2353]],\n",
      "\n",
      "         [[0.0745, 0.0784, 0.0941,  ..., 0.4784, 0.5137, 0.5686],\n",
      "          [0.0863, 0.0863, 0.1529,  ..., 0.4902, 0.5216, 0.6118],\n",
      "          [0.0902, 0.1333, 0.1725,  ..., 0.5137, 0.5569, 0.6196],\n",
      "          ...,\n",
      "          [0.3490, 0.3725, 0.4118,  ..., 0.1412, 0.1176, 0.1098],\n",
      "          [0.5686, 0.3412, 0.2941,  ..., 0.1804, 0.1333, 0.1373],\n",
      "          [0.7451, 0.4549, 0.3098,  ..., 0.2902, 0.2471, 0.2196]]],\n",
      "\n",
      "\n",
      "        [[[0.3059, 0.3843, 0.3765,  ..., 0.5882, 0.5216, 0.5216],\n",
      "          [0.3176, 0.4196, 0.4235,  ..., 0.6039, 0.5333, 0.4824],\n",
      "          [0.3176, 0.4627, 0.4588,  ..., 0.5412, 0.6235, 0.5804],\n",
      "          ...,\n",
      "          [0.5137, 0.4392, 0.4980,  ..., 0.3098, 0.3529, 0.3373],\n",
      "          [0.5137, 0.4314, 0.4980,  ..., 0.2549, 0.3529, 0.3333],\n",
      "          [0.4784, 0.5216, 0.5098,  ..., 0.1647, 0.1843, 0.2471]],\n",
      "\n",
      "         [[0.2157, 0.2392, 0.2275,  ..., 0.4431, 0.3647, 0.3608],\n",
      "          [0.2431, 0.2706, 0.2627,  ..., 0.4627, 0.3882, 0.3255],\n",
      "          [0.2510, 0.2941, 0.2941,  ..., 0.4000, 0.4824, 0.4235],\n",
      "          ...,\n",
      "          [0.3059, 0.2745, 0.3176,  ..., 0.1255, 0.1765, 0.1725],\n",
      "          [0.3059, 0.2627, 0.3176,  ..., 0.0627, 0.1686, 0.1647],\n",
      "          [0.2902, 0.3373, 0.3294,  ..., 0.0980, 0.1059, 0.1412]],\n",
      "\n",
      "         [[0.1529, 0.1647, 0.1529,  ..., 0.3412, 0.2627, 0.2510],\n",
      "          [0.1765, 0.1961, 0.1843,  ..., 0.3686, 0.2863, 0.2235],\n",
      "          [0.1725, 0.2196, 0.2078,  ..., 0.3137, 0.3882, 0.3255],\n",
      "          ...,\n",
      "          [0.1725, 0.2118, 0.2549,  ..., 0.0157, 0.0863, 0.1098],\n",
      "          [0.1725, 0.2275, 0.2667,  ..., 0.0000, 0.0667, 0.0980],\n",
      "          [0.1412, 0.2196, 0.2000,  ..., 0.0706, 0.0784, 0.1059]]]]), tensor([[ 34,  90, 118, 182],\n",
      "        [ 38,  91, 122, 183],\n",
      "        [ 48,  89, 132, 181],\n",
      "        [ 34,  86, 118, 178],\n",
      "        [ 45,  90, 129, 182],\n",
      "        [ 56,  87, 140, 179],\n",
      "        [ 50,  91, 134, 183],\n",
      "        [ 50,  86, 134, 178],\n",
      "        [ 49,  85, 133, 177],\n",
      "        [ 63,  93, 147, 185],\n",
      "        [ 41,  87, 125, 179],\n",
      "        [ 45,  90, 129, 182],\n",
      "        [ 45,  88, 129, 180],\n",
      "        [ 41,  87, 125, 179],\n",
      "        [ 40,  88, 124, 180],\n",
      "        [ 53,  88, 137, 180],\n",
      "        [ 47,  89, 131, 181],\n",
      "        [ 52,  85, 136, 177],\n",
      "        [ 55,  86, 139, 178],\n",
      "        [ 48,  87, 132, 179],\n",
      "        [ 49,  89, 133, 181],\n",
      "        [ 45,  89, 129, 181],\n",
      "        [ 46,  86, 130, 178],\n",
      "        [ 37,  94, 121, 186],\n",
      "        [ 47,  85, 131, 177]])]\n"
     ]
    }
   ],
   "source": [
    "# for x in dataloader:\n",
    "#     print(x[0].shape, x[1].shape, x[2].shape)\n",
    "#     break\n",
    " # Initialize dataset and dataloader\n",
    "\n",
    "full_dataset = FaceDataset('norm_bounding_boxes.csv', '../celeba/img_align_celeba')\n",
    "\n",
    "# Create a subset of the full dataset (100 samples)\n",
    "indices = np.random.permutation(len(dataset))[:100]\n",
    "print(indices)\n",
    "dataset = Subset(full_dataset, indices) \n",
    "\n",
    "# dataset = FaceDataset('norm_bounding_boxes.csv', '../celeba/img_align_celeba')\n",
    "dataloader = DataLoader(dataset, batch_size=25, shuffle=False)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_fg_loss = 0.0\n",
    "#     total_bg_loss = 0.0\n",
    "for x in dataloader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FaceAutoencder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(FaceAutoencder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Maintains 92x84\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 46x42\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Maintains 46x42\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 23x21\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Maintains 23x21\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 11x10\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 11 * 10, latent_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 11 * 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 11, 10)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Outputs 23x21\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Outputs 46x42\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=0, output_padding=1),  # Outputs 92x84\n",
    "            nn.Sigmoid()  # Ensures output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(25, 3, 92, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceAutoencder(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 90, 82])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fg_masks(bboxes, image_height=15, image_width=10):\n",
    "    \"\"\"\n",
    "    Create foreground masks for a batch of images given their bounding boxes, vectorized version.\n",
    "    \"\"\"\n",
    "    batch_size = bboxes.size(0)\n",
    "    # Create coordinate grids\n",
    "    x_coords = torch.arange(image_width).repeat(image_height, 1).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    y_coords = torch.arange(image_height).repeat(image_width, 1).t().unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    # Get bbox coordinates and expand dimensions for broadcasting\n",
    "    lefts = bboxes[:, 0].unsqueeze(1).unsqueeze(2)\n",
    "    tops = bboxes[:, 1].unsqueeze(1).unsqueeze(2)\n",
    "    rights = bboxes[:, 2].unsqueeze(1).unsqueeze(2)\n",
    "    bottoms = bboxes[:, 3].unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # Create masks using logical operations\n",
    "    masks = (x_coords >= lefts) & (x_coords < rights) & (y_coords >= tops) & (y_coords < bottoms)\n",
    "    masks = masks.float().unsqueeze(1)  # Convert from bool to float and add channel dimension\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxs = torch.tensor([\n",
    "[2, 5, 7, 7]\n",
    "])\n",
    "a = create_fg_masks(bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bboxs[0])\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 92, 84]) torch.Size([])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Foreground Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_fg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Background Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_bg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model, model_save_name)\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Combine losses and perform backpropagation\u001b[39;00m\n\u001b[1;32m     89\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m fg_loss \u001b[38;5;241m+\u001b[39m bg_loss\n\u001b[0;32m---> 90\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Aggregate losses for logging\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:259\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    250\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    251\u001b[0m     (inputs,)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    258\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:132\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    136\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from model import TwoResAutoEncoder\n",
    "from data import FaceDataset\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_fg_masks(bboxes, image_height=218, image_width=178):\n",
    "    \"\"\"\n",
    "    Create foreground masks for a batch of images given their bounding boxes, vectorized version.\n",
    "    \"\"\"\n",
    "    batch_size = bboxes.size(0)\n",
    "    # Create coordinate grids\n",
    "    x_coords = torch.arange(image_width).repeat(image_height, 1).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    y_coords = torch.arange(image_height).repeat(image_width, 1).t().unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    # Get bbox coordinates and expand dimensions for broadcasting\n",
    "    lefts = bboxes[:, 0].unsqueeze(1).unsqueeze(2)\n",
    "    tops = bboxes[:, 1].unsqueeze(1).unsqueeze(2)\n",
    "    rights = bboxes[:, 2].unsqueeze(1).unsqueeze(2)\n",
    "    bottoms = bboxes[:, 3].unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # Create masks using logical operations\n",
    "    masks = (x_coords >= lefts) & (x_coords < rights) & (y_coords >= tops) & (y_coords < bottoms)\n",
    "    masks = masks.float().unsqueeze(1)  # Convert from bool to float and add channel dimension\n",
    "\n",
    "    return masks >= 1\n",
    "\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = TwoResAutoEncoder(900, 100)\n",
    "    model.to(torch.device(device))\n",
    "    model.train()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    bg_criterion = nn.MSELoss(reduction='none')\n",
    "    fg_criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    num_epochs = 50\n",
    "    batch_size = 25\n",
    "    dataset_size = 100\n",
    "    model_save_name = \"boiNet.pt\"\n",
    "\n",
    "    # Initialize dataset and dataloader\n",
    "    full_dataset = FaceDataset('norm_bounding_boxes.csv', '../celeba/img_align_celeba')\n",
    "\n",
    "    # Create a subset of the full dataset (100 samples)\n",
    "    indices = np.random.permutation(len(full_dataset))[:1]\n",
    "    dataset = Subset(full_dataset, indices) \n",
    "\n",
    "    # dataset = FaceDataset('norm_bounding_boxes.csv', '../celeba/img_align_celeba')\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_fg_loss = 0.0\n",
    "        total_bg_loss = 0.0\n",
    "        for images, faces, bboxs in dataloader:\n",
    "\n",
    "            # Forward pass\n",
    "            images = images.to(device)\n",
    "            faces = faces.to(device)\n",
    "\n",
    "            fg_masks = create_fg_masks(bboxs)\n",
    "            fg_masks = fg_masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            fg_output, bg_output = model(images, faces)\n",
    "            \n",
    "            breakpoint()\n",
    "            # print(images.shape)\n",
    "            # Calculate the loss for each region\n",
    "            fg_loss = fg_criterion(fg_output, faces)\n",
    "            bg_loss = bg_criterion(bg_output, images) * (~fg_masks)\n",
    "\n",
    "            # Only consider masked areas by averaging non-zero entries\n",
    "            bg_loss = bg_loss.sum() / (~fg_masks).sum()\n",
    "\n",
    "            print(fg_loss.shape, bg_loss.shape)\n",
    "\n",
    "            # Combine losses and perform backpropagation\n",
    "            total_loss = fg_loss + bg_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Aggregate losses for logging\n",
    "            total_fg_loss += fg_loss.item()\n",
    "            total_bg_loss += bg_loss.item()\n",
    "\n",
    "        # Print epoch loss\n",
    "        avg_fg_loss = total_fg_loss / len(dataloader)\n",
    "        avg_bg_loss = total_bg_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Foreground Loss: {avg_fg_loss:.4f}, Background Loss: {avg_bg_loss:.4f}')\n",
    "    \n",
    "    torch.save(model, model_save_name)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftRound(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.round = lambda x: x - torch.sin(2 * torch.pi * x) / (3 * torch.pi)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.round(x * 255)\n",
    "        x = self.round(x)\n",
    "        x = self.round(x)\n",
    "        return x\n",
    "\n",
    "class FGAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(FGAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Maintains 92x84\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 46x42\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Maintains 46x42\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 23x21\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Maintains 23x21\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 11x10\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 11 * 10, latent_dim),\n",
    "            nn.Sigmoid(),\n",
    "            SoftRound()\n",
    "        )\n",
    "        # Decoder\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 11 * 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 11, 10)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output should be ~22x20, adjust if needed\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output should be ~44x40, adjust if needed\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),  # Adjust to output 92x84\n",
    "            nn.Sigmoid()\n",
    "        )    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        print(x)\n",
    "        x = self.decoder(x)\n",
    "        return \n",
    "\n",
    "class FGAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(FGAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Maintains 92x84\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 46x42\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Maintains 46x42\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 23x21\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Maintains 23x21\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 11x10\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 11 * 10, latent_dim),\n",
    "            nn.Sigmoid(),\n",
    "            SoftRound()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 11 * 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 11, 10)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Outputs 23x21\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, output_padding=1),  # Outputs 46x42\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # Outputs 92x84\n",
    "            nn.Sigmoid()  # Ensures output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class BGAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(BGAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Maintains 218x178\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 109x89\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Maintains 109x89\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 54x44\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Maintains 54x44\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduces to 27x22\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 27 * 22, latent_dim),\n",
    "            nn.Sigmoid(),\n",
    "            SoftRound()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 27 * 22),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 27, 22)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Outputs 54x44\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Outputs 109x89\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=0),  # Outputs 218x178\n",
    "            nn.Sigmoid()  # Ensures output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TwoResAutoEncoder(nn.Module):\n",
    "    def __init__(self, high_latent_dim, low_latent_dim):\n",
    "        super(TwoResAutoEncoder, self).__init__()\n",
    "        self.fg_ae = FGAutoencoder(high_latent_dim)\n",
    "        self.bg_ae = BGAutoencoder(low_latent_dim)\n",
    "\n",
    "    def forward(self, image, face):\n",
    "        fg_output = self.fg_ae(face)\n",
    "        bg_output = self.bg_ae(image)\n",
    "        \n",
    "        return fg_output, bg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 92, 84])\n"
     ]
    }
   ],
   "source": [
    "# model = TwoResAutoEncoder(900, 100)\n",
    "\n",
    "# a = torch.randn(1, 3, 92, 84)\n",
    "# b = torch.randn(1, 3, 218, 178)\n",
    "\n",
    "# model(b, a)\n",
    "\n",
    "model = FGAutoencoder(5)\n",
    "\n",
    "a = torch.randn(1, 3, 92, 84)\n",
    "b = torch.randn(1, 3, 218, 178)\n",
    "\n",
    "print(model(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[[[0.6404, 0.4547, 0.3171,  ..., 0.3815, 0.3984, 0.2204],\n",
       "           [0.5142, 0.5858, 0.0777,  ..., 0.5052, 0.3436, 0.5134],\n",
       "           [0.7950, 0.8414, 0.0424,  ..., 0.1109, 0.6439, 0.5930],\n",
       "           ...,\n",
       "           [0.6696, 0.4838, 0.5199,  ..., 0.1338, 0.4587, 0.4222],\n",
       "           [0.6025, 0.4109, 0.6645,  ..., 0.4049, 0.5599, 0.7098],\n",
       "           [0.7121, 0.3255, 0.5862,  ..., 0.5307, 0.5106, 0.8395]],\n",
       " \n",
       "          [[0.4357, 0.7218, 0.1015,  ..., 0.4278, 0.4488, 0.6928],\n",
       "           [0.7315, 0.2358, 0.8995,  ..., 0.4823, 0.4102, 0.5676],\n",
       "           [0.1885, 0.3693, 0.1622,  ..., 0.0860, 0.7474, 0.1691],\n",
       "           ...,\n",
       "           [0.7329, 0.3511, 0.9244,  ..., 0.4755, 0.5593, 0.1928],\n",
       "           [0.5994, 0.2699, 0.6051,  ..., 0.0986, 0.5236, 0.3453],\n",
       "           [0.5247, 0.4191, 0.6504,  ..., 0.3457, 0.1480, 0.2162]],\n",
       " \n",
       "          [[0.4031, 0.7689, 0.7145,  ..., 0.6967, 0.7937, 0.8778],\n",
       "           [0.3278, 0.4516, 0.8619,  ..., 0.5053, 0.8479, 0.7525],\n",
       "           [0.8639, 0.8852, 0.9342,  ..., 0.1145, 0.9591, 0.7791],\n",
       "           ...,\n",
       "           [0.3043, 0.5538, 0.8267,  ..., 0.9265, 0.3299, 0.8839],\n",
       "           [0.6324, 0.6207, 0.6549,  ..., 0.1601, 0.8308, 0.3864],\n",
       "           [0.5568, 0.5901, 0.4238,  ..., 0.8269, 0.3573, 0.5079]]]],\n",
       "        grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
